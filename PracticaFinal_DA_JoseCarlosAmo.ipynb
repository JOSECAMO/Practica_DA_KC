{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkblue\" size=5>**Práctica Final Despliegue de Algoritmos, José Carlos Amo Pérez.**</font>"
      ],
      "metadata": {
        "id": "wZa6Z5ZrawiF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: detección de mensajes troll en chat de Twitch en tiempo real\n",
        "\n",
        "En los últimos años Twitch se ha consolidad como uno de los principales medios de comunicación especialmente para las generaciones más jóvenes.\n",
        "\n",
        "Al tratarse de una plataforma participativa en la que los usuarios pueden poner comentarios durante y posteriormente a las emisiones. Entre estos comentarios han aparecido como siempre comentarios ofensisvos.\n",
        "\n",
        "En esta práctica construiremos una Inteligencia Artificial capaz de clasificar esos mensajes troll.\n",
        "\n",
        "Durante la práctica entrenaremos el modelo de Deep Learning y lo desplegaremos para inferencia en batch, la más habitual actualmente dentro de la industria:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teoría\n",
        "\n",
        "1. **¿Qué es Apache Beam?** Es una plataforma que permiter escribir pipelines de código. La ventaja es que una vez diseñado un modelo de ML, DL, NLP etc se puede automatizar todo el código para su uso en prodcucción. Los pipelines son basicamente concatenaciones de 3 elementos: Un PCollection con los datos de entrada, un PTransform que son las operaciones que transforman esos datos y un nuevo PCollection que contendrá los datos de salida.\n",
        "\n",
        "2. **¿Cuáles son las diferentes formas de desplegar un modelo?** Si por desplegar un modelo entendemos pasar un modelo que ha sido entrenado en un entorno \"de disño\" a un entorno de producción \"para uso real\", existen 4 formas de desplegarlo: \"en batch\" (batch offliune) \"en streaming\" (batch pero online), mediante \"Microservicios/API\" (on demand offline) y \"Machine Learning Automatizado\" (on demand online). La más frecuente hoy en día es en batch y la deseable sería ML Automatizado.\n",
        "\n",
        "3. **¿Cuál es la principal diferencia entre la inferencia en batch y la inferencia en streaming?** La inferencia en batch es offline en el sentido de que los datos se almacenan y tiempo después se procesan. La inferencia en streaming es online o en tiempo real (o casi real) en el sentido de que a medida que llegan datos se procesan."
      ],
      "metadata": {
        "id": "AI5vlvwU9FAv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAorVw7RCT9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5359add0-59e7-44d1-b11e-bcff535aba5b"
      },
      "source": [
        "PROJECT_ID = \"proyecto-jcao-da-kc-2023\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbAl8pSkCXCm"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el bucket mediante la consola y una vez creado lo indicamos en la variable:"
      ],
      "metadata": {
        "id": "p5Xoej7vH0X-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Zpx14FCfXy"
      },
      "source": [
        "BUCKET_NAME = \"bucket-da-2023-jcap\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil mb -l $REGION gs://$BUCKET_NAME   # Esto crea el bucket"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoTc5ge4GS5u",
        "outputId": "da479707-65e6-45a8-992d-ae64de4cd752"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://bucket-da-2023-jcap/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'bucket-da-2023-jcap' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE4V-Xt0ChSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f063e39-f019-453c-b3e4-363965aa5bf2"
      },
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME          # Esto confirma que el bucket está creado ok"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   2756023  2023-03-19T19:02:08Z  gs://bucket-da-2023-jcap/data.json#1679252528447416  metageneration=1\n",
            "TOTAL: 1 objects, 2756023 bytes (2.63 MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "## Preparación\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**, es decir, lo importante no es que nuestro modelo detecte correctamente los tweets de trolls si no que funcione y sea capaz de hacer inferencias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región. Os dejo disponibilizado el dataset en un bucket de acceso público:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqp4L_nmUjAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65cbc170-2c0a-433b-b95e-756abdd4d98b"
      },
      "source": [
        "# Upload data to your bucket\n",
        "! wget https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json -O - | gsutil cp - gs://$BUCKET_NAME/data.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-19 20:07:25--  https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.13.128, 74.125.26.128, 172.217.193.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.13.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                     0%[                    ]       0  --.-KB/s               Copying from <STDIN>...\n",
            "-                   100%[===================>]   2.63M  1.05MB/s    in 2.5s    \n",
            "\n",
            "2023-03-19 20:07:28 (1.05 MB/s) - written to stdout [2756023/2756023]\n",
            "\n",
            "/ [1 files][    0.0 B/    0.0 B]                                                \n",
            "Operation completed over 1 objects.                                              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsblBlJ6RrGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85589c40-89a1-4849-bc2d-b9e6c58c7723"
      },
      "source": [
        "%mkdir /content/batch"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/batch’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ybSaotRwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa31921-2298-45ab-936f-ae7800ae7ed4"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmUJSg1JCgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc6f748-cd86-45f9-83ae-04a620026c54"
      },
      "source": [
        "! wget https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-19 20:07:30--  https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.13.128, 74.125.26.128, 172.217.193.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.13.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘Dataset for Detection of Cyber-Trolls.json.3’\n",
            "\n",
            "\r          Dataset f   0%[                    ]       0  --.-KB/s               \rDataset for Detecti 100%[===================>]   2.63M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-19 20:07:30 (174 MB/s) - ‘Dataset for Detection of Cyber-Trolls.json.3’ saved [2756023/2756023]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s0-8_JK_z2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88595712-532a-40c5-973e-f72951f55c98"
      },
      "source": [
        "#apache-beam[gcp]==2.24.0  Lo comento y lo saco del %%write para resolver error. Cargo beam en el siguiente script a ver si eludo el error\n",
        "%%writefile requirements.txt\n",
        "tensorflow==2.8.0\n",
        "gensim==3.6.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1\n",
        "numpy==1.24.2"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-beam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R11o0B_TClvg",
        "outputId": "ba3d581e-dbee-4742-d214-ace372dd8bd6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: apache-beam in /usr/local/lib/python3.9/dist-packages (2.46.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (2.27.1)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (1.24.2)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (0.18)\n",
            "Requirement already satisfied: httplib2<0.22.0,>=0.8 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (0.21.0)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (2.8.2)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (0.20.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (2.7.0)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (1.51.3)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (0.6.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (2.2.1)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (1.7)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (0.3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (4.5.0)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (2022.10.31)\n",
            "Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (3.19.6)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (1.3.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (3.13.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (1.22.2)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (9.0.0)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.9/dist-packages (from apache-beam) (3.8.7)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.9/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.9/dist-packages (from httplib2<0.22.0,>=0.8->apache-beam) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\"> Resueltos los errores al crear requirements.txt</font>"
      ],
      "metadata": {
        "id": "2a4UJdsiiE-P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvides reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6P8A4c_8le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5200096-92e4-43b3-f506-23740c3a0eb9"
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.8.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: numpy==1.24.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (1.24.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (1.51.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (0.31.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (63.4.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (15.0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (23.3.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0->-r requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0->-r requirements.txt (line 2)) (6.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (3.8.4)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (2.16.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0->-r requirements.txt (line 1)) (0.40.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (4.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0->-r requirements.txt (line 1)) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "## Primer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install contractions  # Lo instalo porque en el preprocesado uso esta funcion para expandir las formas comprimidas del inglés\n",
        "! pip install num2words     # Lo instalo porque en el preprocesado uso esta funcion para pasar numeros a su expresion literal\n",
        "! pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1fx6NDoZYoy",
        "outputId": "6c1e0048-22f6-4dcc-f752-e680c14772b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.9/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.9/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.9/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.9/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.9/dist-packages (0.5.12)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.9/dist-packages (from num2words) (0.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1blctlxs_dPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a462c7-3fdb-4b6f-a519-7b3f6846dfe8"
      },
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import json\n",
        "import contractions\n",
        "\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.coders.coders import Coder\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions, DirectOptions\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from num2words import num2words\n",
        "\n",
        "\n",
        "# CLEANING\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z]+\"  #Elimino también los números\n",
        "\n",
        "\n",
        "class ExtractColumnsDoFn(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        # Parse JSON\n",
        "        data = json.loads(element)\n",
        "        content = data[\"content\"]\n",
        "        label = data[\"annotation\"][\"label\"][0]\n",
        "        result = [content, label]   \n",
        "        yield result\n",
        "\n",
        "\n",
        "class PreprocessColumnsTrainFn(beam.DoFn):\n",
        "    def process_sentiment(self, sentiment):\n",
        "        sentiment = int(sentiment)\n",
        "        if sentiment == 1:\n",
        "            return \"SI_TROLL\"\n",
        "        else:\n",
        "            return \"NO_TROLL\"\n",
        "       \n",
        "    def process_text(self, text):\n",
        "        #Expando las formas contraidas del inglés\n",
        "        text = contractions.fix(text)\n",
        "        # Paso expresiones numericas a su correspondiente literal (por ejemplo 222 -> two hundred and twenty-two)\n",
        "        numbers = re.findall(r'\\d+', text)\n",
        "        for number in numbers:\n",
        "            text = text.replace(number, num2words(int(number)))\n",
        "        # Remove link,user and special characters\n",
        "        stem = False\n",
        "        text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip()\n",
        "        # Elimino espacios en blanco duplicados, es decir: \"estoy    en    casa\" -> \"estoy en casa\"\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "        tokens = []\n",
        "        for token in text.split():\n",
        "            if token not in STOP_WORDS:\n",
        "                if stem:\n",
        "                    tokens.append(STEMMER.stem(token))\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def process(self, element):\n",
        "        processed_text = self.process_text(element[0])\n",
        "        processed_sentiment = self.process_sentiment(element[1])\n",
        "        yield f\"{processed_text}, {processed_sentiment}\"\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(argv=None, save_main_session=True):\n",
        "\n",
        "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\", dest=\"work_dir\", required=True, help=\"Working directory\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--input\", dest=\"input\", required=True, help=\"Input dataset in work dir\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        dest=\"output\",\n",
        "        required=True,\n",
        "        help=\"Output path to store transformed data in work dir\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        dest=\"mode\",\n",
        "        required=True,\n",
        "        choices=[\"train\", \"test\"],\n",
        "        help=\"Type of output to store transformed data\",\n",
        "    )\n",
        "\n",
        "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
        "\n",
        "    # We use the save_main_session option because one or more DoFn's in this\n",
        "    # workflow rely on global context (e.g., a module imported at module level).\n",
        "    pipeline_options = PipelineOptions(pipeline_args)\n",
        "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
        "    pipeline_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "   # The pipeline will be run on exiting the with block.\n",
        "    with beam.Pipeline(options=pipeline_options) as p:\n",
        "\n",
        "        # Read the text file[pattern] into a PCollection.\n",
        "        raw_data = p | \"ReadTwitterData\" >> ReadFromText(\n",
        "            known_args.input, coder=CustomCoder(\"latin-1\")\n",
        "        )\n",
        "\n",
        "        if known_args.mode == \"train\":\n",
        "\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.ParDo(PreprocessColumnsTrainFn())\n",
        "            )\n",
        "\n",
        "            eval_percent = 20\n",
        "            assert 0 < eval_percent < 100, \"eval_percent must in the range (0-100)\"\n",
        "            train_dataset, eval_dataset = (\n",
        "                transformed_data\n",
        "                | \"Split dataset\"\n",
        "                >> beam.Partition(\n",
        "                    lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2\n",
        "                )\n",
        "            )\n",
        "\n",
        "            train_dataset | \"TrainWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"train\", \"part\")\n",
        "            )\n",
        "            eval_dataset | \"EvalWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"eval\", \"part\")\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.Map(lambda x: f'\"{x[0]}\"')\n",
        "            )\n",
        "\n",
        "            transformed_data | \"TestWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"test\", \"part\")\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocess.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpirg37C3bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a42be1-cd01-4143-e55b-1c27512d0fcf"
      },
      "source": [
        "# Run este script para el preprocesado en local con los datos de entrenamiento\n",
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/Dataset*.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 2 ; running_mode: in_memory\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f6c95866f70> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f6c958220d0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f6c958225e0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f6c95822670> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f6c95822820> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f6c958228b0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f6c958229d0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f6c95822a60> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f6c95822af0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f6c95822b80> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f6c95822dc0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f6c95822ee0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f6c95822d30> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f6c95822e50> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f6c9561f220> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f6c9561fe50> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 2 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 2 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.01 seconds.\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.01 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local \n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4lkLgecLS3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358a079e-3c8a-49d4-8810-d5f7f9c63912"
      },
      "source": [
        "# Run este script para el preprocesado en local con los datos de test\n",
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/Dataset*.json\\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 2 ; running_mode: in_memory\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f35e649de50> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f35e649df70> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f35e649e4c0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f35e649e550> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f35e649e700> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f35e649e790> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f35e649e8b0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f35e649e940> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f35e649e9d0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f35e649ea60> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f35e649eca0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f35e649edc0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f35e649ec10> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f35e649ed30> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f35e64419a0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f35e6429ac0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 2 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.46.0\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.00 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Segundo ejercicio\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMi8dI1gLoIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918e5ccf-a383-4632-e964-0455e0ea7368"
      },
      "source": [
        "%mkdir /content/batch/trainer"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/batch/trainer’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJyMXTuNPwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35acf9bd-ddf6-4b3c-a270-d5c8611357dd"
      },
      "source": [
        "%%writefile trainer/__init__.py\n",
        "\n",
        "version = \"0.1.0\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trainer/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUu6gKaTL_S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cdfbc2-23c0-40a8-f706-7ecacf105b76"
      },
      "source": [
        "%%writefile trainer/task.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        ")\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 300 #tamaño del embedding\n",
        "W2V_WINDOW = 7 #tamaño de la ventana\n",
        "# 32\n",
        "W2V_EPOCH = 5 #epochs para entrenar\n",
        "W2V_MIN_COUNT = 10 #se eliminan palabras que aparecen menos de 10 veces\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300 #longitud de secuencia para el padding\n",
        "\n",
        "# SENTIMENT definimos las etiquetas\n",
        "NEGATIVE = \"NO_TROLL\"\n",
        "POSITIVE = \"SI_TROLL\"\n",
        "SENTIMENT_THRESHOLDS = [0.5, 0.5]\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\" #nombre del archivo del modelo\n",
        "WORD2VEC_MODEL = \"model.w2v\" #nombre del archivo del embedding\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\" #nombre del tokenizador\n",
        "ENCODER_MODEL = \"encoder.pkl\" #nombre del encoder\n",
        "\n",
        "# TRAIN AND EVALUATE\n",
        "STEPS = 10   # Pongo un valor muy bajo porque me he quedado sin GPUs y si subo es eterno\n",
        "\n",
        "def generate_word2vec(train_df):\n",
        "  ### Genera el embedding y lo entrena\n",
        "    documents = [_text.split() for _text in train_df.text.values]\n",
        "    w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "        size=W2V_SIZE,\n",
        "        window=W2V_WINDOW,\n",
        "        min_count=W2V_MIN_COUNT,\n",
        "        workers=mp.cpu_count(),\n",
        "    )\n",
        "    w2v_model.build_vocab(documents)\n",
        "\n",
        "    words = w2v_model.wv.vocab.keys()\n",
        "    vocab_size = len(words)\n",
        "    logging.info(f\"Vocab size: {vocab_size}\")\n",
        "    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "    return w2v_model\n",
        "\n",
        "\n",
        "def generate_tokenizer(train_df):\n",
        "  ### Genera nuestro tokenizador\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_df.text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    logging.info(f\"Total words: {vocab_size}\")\n",
        "    return tokenizer, vocab_size\n",
        "\n",
        "\n",
        "def generate_label_encoder(train_df):\n",
        "  ### COdifica las etiquetas (de texto a número)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_df.sentiment.tolist())\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
        "  ### Genera el embedding en base al Word2Vec\n",
        "    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    return Embedding(\n",
        "        vocab_size,\n",
        "        W2V_SIZE,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        trainable=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_and_evaluate(\n",
        "    work_dir, train_df, eval_df, batch_size=1024, epochs=8, steps=STEPS\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains and evaluates the estimator given.\n",
        "    The input functions are generated by the preprocessing function.\n",
        "    \"\"\"\n",
        "\n",
        "    model_dir = os.path.join(work_dir, \"model\") # Comprobamos si ya existe un modelo\n",
        "    if tf.io.gfile.exists(model_dir):\n",
        "        tf.io.gfile.rmtree(model_dir) #si existe lo eliminamos\n",
        "    tf.io.gfile.mkdir(model_dir) #creamos un directorio de modelo\n",
        "\n",
        "    # Configuramos donde guardar el modelo\n",
        "    run_config = tf.estimator.RunConfig() \n",
        "    run_config = run_config.replace(model_dir=model_dir)\n",
        "\n",
        "    # Nos permite seguir el entrenamiento cada 10 steps\n",
        "    run_config = run_config.replace(save_summary_steps=10)\n",
        "\n",
        "    # Generamos el Word2Vec para entrenamiento\n",
        "    logging.info(\"---- Generating word2vec model ----\")\n",
        "    word2vec_model = generate_word2vec(train_df) \n",
        "\n",
        "    # Generamos el tokenizador con el dato de entrenamiento e inferimos en el de entrenamiento y evaluación\n",
        "    logging.info(\"---- Generating tokenizer ----\")\n",
        "    train_df = pd.concat([train_df] * 7, ignore_index=True)   # Para asegurarme de que el modelo vea suficientes ejemplos de entrenamiento\n",
        "    tokenizer, vocab_size = generate_tokenizer(train_df)\n",
        "\n",
        "    logging.info(\"---- Tokenizing train data ----\")\n",
        "    x_train = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    ) #Añadimos el padding\n",
        "    logging.info(\"---- Tokenizing eval data ----\")\n",
        "    x_eval = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "\n",
        "    # Generamos los encodings de las etiquetas tanto para entrenamiento como para evaluación\n",
        "    logging.info(\"---- Generating label encoder ----\")\n",
        "    label_encoder = generate_label_encoder(train_df)\n",
        "\n",
        "    logging.info(\"---- Encoding train target ----\")\n",
        "    y_train = label_encoder.transform(train_df.sentiment.tolist())\n",
        "    logging.info(\"---- Encoding eval target ----\")\n",
        "    y_eval = label_encoder.transform(eval_df.sentiment.tolist())\n",
        "\n",
        "    y_train = y_train.reshape(-1, 1) #adaptamos la forma del tensor\n",
        "    y_eval = y_eval.reshape(-1, 1)\n",
        "\n",
        "    # Create Embedding Layer\n",
        "    logging.info(\"---- Generating embedding layer ----\")\n",
        "    embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer) #capade embedding\n",
        "    # Construimos nuestra red LSTM\n",
        "    logging.info(\"---- Generating Sequential model ----\")\n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    logging.info(\"---- Adding loss function to model ----\")\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) #problema binario y medimos según el accuracy\n",
        "\n",
        "    logging.info(\"---- Adding callbacks to model ----\")\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, cooldown=0), \n",
        "        EarlyStopping(monitor=\"val_accuracy\", min_delta=1e-4, patience=5), #early stopping para agilizar\n",
        "    ]\n",
        "\n",
        "    logging.info(\"---- Training model ----\")\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "    ) #entrenamos nuestro modelo\n",
        "\n",
        "    logging.info(\"---- Evaluating model ----\")\n",
        "    score = model.evaluate(x_eval, y_eval, batch_size=batch_size) #evaluamos el modelo\n",
        "    logging.info(f\"ACCURACY: {score[1]}\")\n",
        "    logging.info(f\"LOSS: {score[0]}\")\n",
        "\n",
        "    logging.info(\"---- Saving models ----\")\n",
        "    pickle.dump(\n",
        "        tokenizer,\n",
        "        tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode=\"wb\"),\n",
        "        protocol=0,\n",
        "    ) #guardamos el tokenizador\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "        with tf.io.gfile.GFile(\n",
        "            os.path.join(model_dir, KERAS_MODEL), mode=\"wb\" #guardamos el archivo\n",
        "        ) as gcs_file:\n",
        "            model.save(local_file.name)\n",
        "            gcs_file.write(local_file.read())\n",
        "\n",
        "    # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))\n",
        "\n",
        "    # pickle.dump(\n",
        "    #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), \"wb\"), protocol=0\n",
        "    # )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"Main function called by AI Platform.\"\"\"\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO) # Activamos el logger\n",
        "\n",
        "    parser = argparse.ArgumentParser( #Implementamos el parseador de argumentos. Ver script de preprocesamiento para más detalle\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"Directory for staging trainer files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for staging and working files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=1024,\n",
        "        help=\"Batch size for training and evaluation.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=8, help=\"Number of epochs to train the model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--steps\",\n",
        "        type=int,\n",
        "        default=STEPS,\n",
        "        help=\"Number of steps per epoch to train the model\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args() #cargamos todos los argumentos antes mencionados\n",
        "\n",
        "    train_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"/content/batch/transformed_data/train/part-*\") # el * permite que lea todas las particiones\n",
        "    ) #archivos de entrenamiento\n",
        "    eval_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"/content/batch/transformed_data/eval/part-*\")\n",
        "    ) #archivos de test\n",
        "\n",
        "    train_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            ) # leemos cada csv \n",
        "            for f in train_data_files\n",
        "        ]\n",
        "    ).dropna() #generamos un dataframe con todas las particiones\n",
        "\n",
        "    eval_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in eval_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    train_and_evaluate(\n",
        "        args.work_dir,\n",
        "        train_df=train_df,\n",
        "        eval_df=eval_df,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        steps=args.steps,\n",
        "    )\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trainer/task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9997ZzsLmq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494bb7fc-cb48-43bc-b9c7-ee5b4e44c6f9"
      },
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name trainer.task \\\n",
        "  -- \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --epochs 1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [ml_engine/local_python].\n",
            "INFO:tensorflow:TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--work-dir', '/content/batch', '--epochs', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}\n",
            "INFO:root:---- Generating word2vec model ----\n",
            "INFO:gensim.models.word2vec:collecting all words and their counts\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 66901 words, keeping 11467 word types\n",
            "INFO:gensim.models.word2vec:collected 14439 word types from a corpus of 113404 raw words and 15990 sentences\n",
            "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
            "INFO:gensim.models.word2vec:effective_min_count=10 retains 1559 unique words (10% of original 14439, drops 12880)\n",
            "INFO:gensim.models.word2vec:effective_min_count=10 leaves 84461 word corpus (74% of original 113404, drops 28943)\n",
            "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 14439 items\n",
            "INFO:gensim.models.word2vec:sample=0.001 downsamples 56 most-common words\n",
            "INFO:gensim.models.word2vec:downsampling leaves estimated 69522 word corpus (82.3% of prior 84461)\n",
            "INFO:gensim.models.base_any2vec:estimated required memory for 1559 words and 300 dimensions: 4521100 bytes\n",
            "INFO:gensim.models.word2vec:resetting layer weights\n",
            "INFO:root:Vocab size: 1559\n",
            "INFO:gensim.models.base_any2vec:training model with 2 workers on 1559 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 113404 raw words (69392 effective words) took 0.2s, 358087 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 113404 raw words (69637 effective words) took 0.2s, 417584 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 113404 raw words (69434 effective words) took 0.2s, 462556 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 113404 raw words (69511 effective words) took 0.2s, 443811 effective words/s\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
            "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
            "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 113404 raw words (69574 effective words) took 0.2s, 453746 effective words/s\n",
            "INFO:gensim.models.base_any2vec:training on a 567020 raw words (347548 effective words) took 0.9s, 407536 effective words/s\n",
            "INFO:root:---- Generating tokenizer ----\n",
            "INFO:root:Total words: 14440\n",
            "INFO:root:---- Tokenizing train data ----\n",
            "INFO:root:---- Tokenizing eval data ----\n",
            "INFO:root:---- Generating label encoder ----\n",
            "INFO:root:---- Encoding train target ----\n",
            "INFO:root:---- Encoding eval target ----\n",
            "INFO:root:---- Generating embedding layer ----\n",
            "INFO:root:---- Generating Sequential model ----\n",
            "2023-03-19 20:09:09.171790: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 300)          4332000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 300, 300)          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,492,501\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 4,332,000\n",
            "_________________________________________________________________\n",
            "INFO:root:---- Adding loss function to model ----\n",
            "INFO:root:---- Adding callbacks to model ----\n",
            "INFO:root:---- Training model ----\n",
            " 6/10 [=================>............] - ETA: 1:12 - loss: 0.6898 - accuracy: 0.5775"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Tercer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los datos de test generados en el primer ejercicio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHBpLXB-OmjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f148ec70-cdae-4a7c-c644-5feeced9d286"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "from apache_beam.coders.coders import Coder\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "NEGATIVE = \"NO_TROLL\"  # Esta es una posible clasificacion\n",
        "POSITIVE = \"SI_TROLL\"  # Esta es la otra\n",
        "SENTIMENT_THRESHOLDS = [0.5, 0.5] # Lo que salga por debajo de 0.5 se clasifica como negativo (NO_TROLL)\n",
        "                                  # y lo que salga por encima como positivo (SI_TROLL)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "\n",
        "\n",
        "class Predict(beam.DoFn): #realiza predicciones y hereda de Beam\n",
        "    def __init__(\n",
        "        self, model_dir,\n",
        "    ):\n",
        "        self.model_dir = model_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def setup(self): #es un método de apache Beam que se ejecuta la primera vez que se ejecute e inicializa el modelo de Keras y el tokenizador cargándolos\n",
        "        keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "            with tf.io.gfile.GFile(keras_model_path, mode=\"rb\") as gcs_file:\n",
        "                local_file.write(gcs_file.read())\n",
        "                self.model = tf.keras.models.load_model(local_file.name)\n",
        "\n",
        "        tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)\n",
        "        self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode=\"rb\"))\n",
        "\n",
        "    def decode_sentiment(self, score, include_neutral=True):\n",
        "#        if include_neutral:\n",
        "#           label = NEUTRAL\n",
        "#            if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "#                label = NEGATIVE\n",
        "#            elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "#                label = POSITIVE\n",
        "#\n",
        "#            return label\n",
        "#        else:\n",
        "            return NEGATIVE if score < 0.5 else POSITIVE\n",
        "\n",
        "    def process(self, element): #recoge elt exto a predecir, tokeniza \n",
        "        start_at = time.time()\n",
        "        # Tokenize text\n",
        "        x_test = pad_sequences(\n",
        "            self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH \n",
        "        ) #tokenizado y padding\n",
        "        # Predict\n",
        "        score = self.model.predict([x_test])[0] #realiza la inferencia\n",
        "        # Decode sentiment\n",
        "        label = self.decode_sentiment(score) #trasnforma la predicción en sentimiento\n",
        "\n",
        "        yield {\n",
        "            \"text\": element,\n",
        "            \"label\": label,\n",
        "            \"score\": float(score),\n",
        "            \"elapsed_time\": time.time() - start_at,\n",
        "        } #devuelve un diccionario con la información clave\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(model_dir, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p #generamos el pipeline\n",
        "            | \"Read data\" >> source #lectura de datos\n",
        "            | \"Predict\" >> beam.ParDo(Predict(model_dir)) #hacemos la predicción\n",
        "            | \"Format as JSON\" >> beam.Map(json.dumps) #convertimos el diccionario en string\n",
        "            | \"Write predictions\" >> sink #guardamos la información\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter # parseador de argumentos\n",
        "    )\n",
        "\n",
        "    parser.add_argument( #directorio de trabajo\n",
        "        \"--work-dir\",\n",
        "        dest=\"work_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument( #ubicación del modelo para inferencia\n",
        "        \"--model-dir\",\n",
        "        dest=\"model_dir\",\n",
        "        required=True,\n",
        "        help=\"Path to the exported TensorFlow model. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    verbs = parser.add_subparsers(dest=\"verb\")\n",
        "    batch_verb = verbs.add_parser(\"batch\", help=\"Batch prediction\")\n",
        "    batch_verb.add_argument( #datos de entrada para la predicción\n",
        "        \"--inputs-dir\",\n",
        "        dest=\"inputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Input directory where CSV data files are read from. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "    batch_verb.add_argument( #datos de salida para la predicción\n",
        "        \"--outputs-dir\",\n",
        "        dest=\"outputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory to store prediction results. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    print(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if args.verb == \"batch\": #si es batch genera transformadores iniciales, la fuente y la salida de datos\n",
        "        results_prefix = os.path.join(args.outputs_dir, \"part\")\n",
        "\n",
        "        source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n",
        "        sink = WriteToText(results_prefix)\n",
        "\n",
        "    else: # de momento no funciona si no es batch\n",
        "        parser.print_usage()\n",
        "        sys.exit(1)\n",
        "\n",
        "    run(args.model_dir, source, sink, beam_options)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pvKMtuQPOlr"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJN0XCLPR_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62d2042-7150-456b-80bb-f035bd854561"
      },
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/model \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(work_dir='/content/batch', model_dir='/content/batch/model', verb='batch', inputs_dir='/content/batch/transformed_data/test/part-00000-of-00002', outputs_dir='/content/batch/predictions/2023-03-19_20-11-07')\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/content/batch/transformed_data/test/part-00001-of-00002']\n",
            "Traceback (most recent call last):\n",
            "  File \"apache_beam/runners/common.py\", line 1466, in apache_beam.runners.common.DoFnRunner._invoke_lifecycle_method\n",
            "  File \"apache_beam/runners/common.py\", line 552, in apache_beam.runners.common.DoFnInvoker.invoke_setup\n",
            "  File \"/content/batch/predict.py\", line 50, in setup\n",
            "    local_file.write(gcs_file.read())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/lib/io/file_io.py\", line 114, in read\n",
            "    self._preread_check()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/lib/io/file_io.py\", line 76, in _preread_check\n",
            "    self._read_buf = _pywrap_file_io.BufferedInputStream(\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: /content/batch/model/model.h5; No such file or directory\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/predict.py\", line 173, in <module>\n",
            "    run(args.model_dir, source, sink, beam_options)\n",
            "  File \"/content/batch/predict.py\", line 107, in run\n",
            "    _ = (\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/pipeline.py\", line 600, in __exit__\n",
            "    self.result = self.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/pipeline.py\", line 577, in run\n",
            "    return self.runner.run_pipeline(self, self._options)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/direct/direct_runner.py\", line 131, in run_pipeline\n",
            "    return runner.run_pipeline(pipeline, options)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 202, in run_pipeline\n",
            "    self._latest_run_result = self.run_via_runner_api(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 224, in run_via_runner_api\n",
            "    return self.run_stages(stage_context, stages)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 455, in run_stages\n",
            "    bundle_results = self._execute_bundle(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 783, in _execute_bundle\n",
            "    self._run_bundle(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1012, in _run_bundle\n",
            "    result, splits = bundle_manager.process_bundle(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\", line 1348, in process_bundle\n",
            "    result_future = self._worker_handler.control_conn.push(process_bundle_req)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py\", line 379, in push\n",
            "    response = self.worker.do_instruction(request)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/worker/sdk_worker.py\", line 624, in do_instruction\n",
            "    return getattr(self, request_type)(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/worker/sdk_worker.py\", line 655, in process_bundle\n",
            "    bundle_processor = self.bundle_processor_cache.get(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/worker/sdk_worker.py\", line 486, in get\n",
            "    processor = bundle_processor.BundleProcessor(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/runners/worker/bundle_processor.py\", line 906, in __init__\n",
            "    op.setup()\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 833, in apache_beam.runners.worker.operations.DoOperation.setup\n",
            "  File \"apache_beam/runners/worker/operations.py\", line 882, in apache_beam.runners.worker.operations.DoOperation.setup\n",
            "  File \"apache_beam/runners/common.py\", line 1472, in apache_beam.runners.common.DoFnRunner.setup\n",
            "  File \"apache_beam/runners/common.py\", line 1468, in apache_beam.runners.common.DoFnRunner._invoke_lifecycle_method\n",
            "  File \"apache_beam/runners/common.py\", line 1508, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
            "  File \"apache_beam/runners/common.py\", line 1466, in apache_beam.runners.common.DoFnRunner._invoke_lifecycle_method\n",
            "  File \"apache_beam/runners/common.py\", line 552, in apache_beam.runners.common.DoFnInvoker.invoke_setup\n",
            "  File \"/content/batch/predict.py\", line 50, in setup\n",
            "    local_file.write(gcs_file.read())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/lib/io/file_io.py\", line 114, in read\n",
            "    self._preread_check()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/lib/io/file_io.py\", line 76, in _preread_check\n",
            "    self._read_buf = _pywrap_file_io.BufferedInputStream(\n",
            "RuntimeError: tensorflow.python.framework.errors_impl.NotFoundError: /content/batch/model/model.h5; No such file or directory [while running 'Predict']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um68jx_F8mjF"
      },
      "source": [
        "## Extra. Comprobaciones en la nube\n",
        "\n",
        "En esta última parte se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHC9dCT83FH"
      },
      "source": [
        "Establecemos el bucket y region de GCP sobre el que trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUpX__KQvt-"
      },
      "source": [
        "# GCP_WORK_DIR = ''\n",
        "# GCP_REGION = ''\n",
        "\n",
        "BUCKET_NAME = \"bucket-da-2023-jcap\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MQvWsk_mVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6bf5df8-86fb-4b03-98b7-96945aa6baf3"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"apache-beam[gcp]==2.24.0\",\n",
        "  \"tensorflow==2.8.0\"\n",
        "  \"gensim==3.6.0\"\n",
        "  \"fsspec==0.8.4\"\n",
        "  \"gcsfs==0.7.1\"\n",
        "  \"numpy==1.24.2\"\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Habría que parsear lo siguiente:\n",
        "\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\"
      ],
      "metadata": {
        "id": "GO7GTW_lehHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run este script para el preprocesado en ****Google Cloud***** con los datos de entrenamiento\n",
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --input $WORK_DIR/Dataset*.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "metadata": {
        "id": "0HbyIHq_sn9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2685f95a-6624-4daa-cbd6-97770133931c"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:apache_beam.options.pipeline_options:Unable to create default GCS bucket.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/preprocess.py\", line 182, in <module>\n",
            "    run()\n",
            "  File \"/content/batch/preprocess.py\", line 136, in run\n",
            "    with beam.Pipeline(options=pipeline_options) as p:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/apache_beam/pipeline.py\", line 203, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Pipeline has validations errors: \n",
            "Invalid GCS path (/content/batch/beam-temp), given for the option: temp_location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run este script para el preprocesado en *******google Cloud*********** con los datos de test\n",
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --input $WORK_DIR/data/Dataset*.json \\\n",
        "  --output $WORK_DIR/data/transformed_data \\\n",
        "  --mode test"
      ],
      "metadata": {
        "id": "sFTc9PhbsuuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A66WAfm9DmU"
      },
      "source": [
        "### Validación preprocess train en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYVuWyJP1Ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866f81c0-0551-4034-b0c2-89ba845ead66"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/preprocess.py\", line 3, in <module>\n",
            "    import apache_beam as beam\n",
            "ModuleNotFoundError: No module named 'apache_beam'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czKRMb99ZRX"
      },
      "source": [
        "### Validación preprocess test en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45mfSRGdQLBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc9c113-bfb5-4682-944e-2efe53d65b29"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/preprocess.py\", line 3, in <module>\n",
            "    import apache_beam as beam\n",
            "ModuleNotFoundError: No module named 'apache_beam'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omitimos el entrenamiento en la nube para no incurrir en costes innecesarios."
      ],
      "metadata": {
        "id": "sTVj6bqII_3m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5SOWsy9886"
      },
      "source": [
        "### Validación predict en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXoHCgMg-LUD"
      },
      "source": [
        "Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcetQtnQjVo"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT_NxsKDQlxD"
      },
      "source": [
        "# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n",
        "! python3 predict.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --model-dir $GCP_WORK_DIR/model/ \\\n",
        "  batch \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}